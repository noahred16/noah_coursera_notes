{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-29 20:43:17.637520: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-29 20:43:17.639518: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-29 20:43:17.688526: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-29 20:43:17.689346: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-29 20:43:18.774598: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.losses import mean_squared_error\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tensorflow.keras.layers import Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_housing_data(num_points=2000):\n",
    "    # Generate random data for square footage and number of bedrooms\n",
    "    x1 = np.random.randint(low=500, high=5000, size=num_points)\n",
    "    x2 = np.random.randint(low=1, high=6, size=num_points)\n",
    "    x = np.column_stack((x1, x2))  # Combine the two arrays into a single 2D array\n",
    "\n",
    "    # Generate random prices for each data point\n",
    "    noise = np.random.normal(loc=0.0, scale=1.0, size=num_points)  # Add some random noise to the prices\n",
    "    y = 200 * x1 + 100 * x2 + 50000 + 100000 * noise\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape is  (2000, 2)\n",
      "first 5 rows are \n",
      " [[3258    3]\n",
      " [ 539    5]\n",
      " [ 801    5]\n",
      " [2548    5]\n",
      " [4330    1]]\n",
      "min values are  [500   1]\n",
      "max values are  [4999    5]\n",
      "-\n",
      "shape is  (2000, 2)\n",
      "first 5 rows are \n",
      " [[3.06512558 2.5       ]\n",
      " [0.04334297 5.        ]\n",
      " [0.33451878 5.        ]\n",
      " [2.27606135 5.        ]\n",
      " [4.25650144 0.        ]]\n",
      "min values are  [0. 0.]\n",
      "max values are  [5. 5.]\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "def print_info(x):\n",
    "    print('shape is ', x.shape)\n",
    "    print('first 5 rows are \\n', x[:5])\n",
    "    print('min values are ', np.min(x, axis=0))\n",
    "    print('max values are ', np.max(x, axis=0))\n",
    "    print('-')\n",
    "    \n",
    "x_train, y_train = generate_housing_data()\n",
    "x_test, y_test = generate_housing_data(50)\n",
    "\n",
    "\n",
    "print_info(x_train)\n",
    "\n",
    "min_vals = np.min(x_train, axis=0)\n",
    "max_vals = np.max(x_train, axis=0)\n",
    "\n",
    "normalized_x_train = 5* (x_train - min_vals) / (max_vals - min_vals)\n",
    "\n",
    "print_info(normalized_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tf.keras.Input(shape=(400,)),\n",
    "model = Sequential([\n",
    "        tf.keras.Input(shape=(2,)),\n",
    "        # Dense(units=15, activation='relu'),\n",
    "        Dense(units=5, activation='relu'),\n",
    "        Dense(units=1, activation='linear')\n",
    "    ])\n",
    "\n",
    "\n",
    "# model = Sequential([\n",
    "#     Dense(units=32, activation='relu', input_dim=2),\n",
    "#     Dropout(0.2),\n",
    "#     Dense(units=1, activation='relu')\n",
    "# ])\n",
    "\n",
    "optimizer = Adam(learning_rate=0.01)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=mean_squared_error\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "63/63 [==============================] - 1s 3ms/step - loss: 445307224064.0000\n",
      "Epoch 2/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 445259808768.0000\n",
      "Epoch 3/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 445167173632.0000\n",
      "Epoch 4/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 445006675968.0000\n",
      "Epoch 5/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 444760555520.0000\n",
      "Epoch 6/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 444439330816.0000\n",
      "Epoch 7/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 444048932864.0000\n",
      "Epoch 8/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 443592966144.0000\n",
      "Epoch 9/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 443073527808.0000\n",
      "Epoch 10/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 442492715008.0000\n",
      "Epoch 11/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 441853509632.0000\n",
      "Epoch 12/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 441157287936.0000\n",
      "Epoch 13/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 440405458944.0000\n",
      "Epoch 14/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 439596548096.0000\n",
      "Epoch 15/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 438736748544.0000\n",
      "Epoch 16/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 437821472768.0000\n",
      "Epoch 17/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 436860223488.0000\n",
      "Epoch 18/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 435847528448.0000\n",
      "Epoch 19/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 434790400000.0000\n",
      "Epoch 20/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 433685954560.0000\n",
      "Epoch 21/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 432536387584.0000\n",
      "Epoch 22/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 431345139712.0000\n",
      "Epoch 23/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 430106640384.0000\n",
      "Epoch 24/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 428827934720.0000\n",
      "Epoch 25/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 427507843072.0000\n",
      "Epoch 26/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 426152722432.0000\n",
      "Epoch 27/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 424758116352.0000\n",
      "Epoch 28/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 423326384128.0000\n",
      "Epoch 29/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 421858246656.0000\n",
      "Epoch 30/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 420352491520.0000\n",
      "Epoch 31/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 418812952576.0000\n",
      "Epoch 32/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 417240776704.0000\n",
      "Epoch 33/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 415635505152.0000\n",
      "Epoch 34/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 413994909696.0000\n",
      "Epoch 35/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 412326985728.0000\n",
      "Epoch 36/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 410624032768.0000\n",
      "Epoch 37/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 408897060864.0000\n",
      "Epoch 38/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 407140302848.0000\n",
      "Epoch 39/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 405352251392.0000\n",
      "Epoch 40/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 403533201408.0000\n",
      "Epoch 41/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 401690361856.0000\n",
      "Epoch 42/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 399824191488.0000\n",
      "Epoch 43/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 397931216896.0000\n",
      "Epoch 44/200\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 396014125056.0000\n",
      "Epoch 45/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 394074488832.0000\n",
      "Epoch 46/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 392106311680.0000\n",
      "Epoch 47/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 390113394688.0000\n",
      "Epoch 48/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 388102946816.0000\n",
      "Epoch 49/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 386068873216.0000\n",
      "Epoch 50/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 384014090240.0000\n",
      "Epoch 51/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 381938434048.0000\n",
      "Epoch 52/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 379841740800.0000\n",
      "Epoch 53/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 377724502016.0000\n",
      "Epoch 54/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 375586488320.0000\n",
      "Epoch 55/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 373433237504.0000\n",
      "Epoch 56/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 371259375616.0000\n",
      "Epoch 57/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 369069064192.0000\n",
      "Epoch 58/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 366857519104.0000\n",
      "Epoch 59/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 364633653248.0000\n",
      "Epoch 60/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 362387570688.0000\n",
      "Epoch 61/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 360132575232.0000\n",
      "Epoch 62/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 357861490688.0000\n",
      "Epoch 63/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 355574611968.0000\n",
      "Epoch 64/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 353272889344.0000\n",
      "Epoch 65/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 350958157824.0000\n",
      "Epoch 66/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 348629794816.0000\n",
      "Epoch 67/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 346285998080.0000\n",
      "Epoch 68/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 343927488512.0000\n",
      "Epoch 69/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 341563604992.0000\n",
      "Epoch 70/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 339182354432.0000\n",
      "Epoch 71/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 336795500544.0000\n",
      "Epoch 72/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 334398291968.0000\n",
      "Epoch 73/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 331982307328.0000\n",
      "Epoch 74/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 329560948736.0000\n",
      "Epoch 75/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 327130710016.0000\n",
      "Epoch 76/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 324683857920.0000\n",
      "Epoch 77/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 322229370880.0000\n",
      "Epoch 78/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 319770591232.0000\n",
      "Epoch 79/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 317298180096.0000\n",
      "Epoch 80/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 314822950912.0000\n",
      "Epoch 81/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 312339562496.0000\n",
      "Epoch 82/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 309844148224.0000\n",
      "Epoch 83/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 307346374656.0000\n",
      "Epoch 84/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 304835526656.0000\n",
      "Epoch 85/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 302329364480.0000\n",
      "Epoch 86/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 299812651008.0000\n",
      "Epoch 87/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 297292529664.0000\n",
      "Epoch 88/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 294770311168.0000\n",
      "Epoch 89/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 292244717568.0000\n",
      "Epoch 90/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 289708769280.0000\n",
      "Epoch 91/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 287174295552.0000\n",
      "Epoch 92/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 284633595904.0000\n",
      "Epoch 93/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 282092109824.0000\n",
      "Epoch 94/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 279551934464.0000\n",
      "Epoch 95/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 277012905984.0000\n",
      "Epoch 96/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 274459770880.0000\n",
      "Epoch 97/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 271915745280.0000\n",
      "Epoch 98/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 269361233920.0000\n",
      "Epoch 99/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 266810703872.0000\n",
      "Epoch 100/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 264263958528.0000\n",
      "Epoch 101/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 261719212032.0000\n",
      "Epoch 102/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 259164864512.0000\n",
      "Epoch 103/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 256626622464.0000\n",
      "Epoch 104/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 254071750656.0000\n",
      "Epoch 105/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 251520729088.0000\n",
      "Epoch 106/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 248979439616.0000\n",
      "Epoch 107/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 246437167104.0000\n",
      "Epoch 108/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 243906609152.0000\n",
      "Epoch 109/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 241373331456.0000\n",
      "Epoch 110/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 238839742464.0000\n",
      "Epoch 111/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 236310593536.0000\n",
      "Epoch 112/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 233792487424.0000\n",
      "Epoch 113/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 231276265472.0000\n",
      "Epoch 114/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 228768088064.0000\n",
      "Epoch 115/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 226262269952.0000\n",
      "Epoch 116/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 223765676032.0000\n",
      "Epoch 117/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 221267787776.0000\n",
      "Epoch 118/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 218784694272.0000\n",
      "Epoch 119/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 216298782720.0000\n",
      "Epoch 120/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 213823946752.0000\n",
      "Epoch 121/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 211361136640.0000\n",
      "Epoch 122/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 208904192000.0000\n",
      "Epoch 123/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 206457962496.0000\n",
      "Epoch 124/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 204017729536.0000\n",
      "Epoch 125/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 201593143296.0000\n",
      "Epoch 126/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 199176372224.0000\n",
      "Epoch 127/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 196776099840.0000\n",
      "Epoch 128/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 194372861952.0000\n",
      "Epoch 129/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 191987302400.0000\n",
      "Epoch 130/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 189615833088.0000\n",
      "Epoch 131/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 187253407744.0000\n",
      "Epoch 132/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 184897880064.0000\n",
      "Epoch 133/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 182560096256.0000\n",
      "Epoch 134/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 180237615104.0000\n",
      "Epoch 135/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 177915576320.0000\n",
      "Epoch 136/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 175610413056.0000\n",
      "Epoch 137/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 173330432000.0000\n",
      "Epoch 138/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 171054153728.0000\n",
      "Epoch 139/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 168798846976.0000\n",
      "Epoch 140/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 166559416320.0000\n",
      "Epoch 141/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 164327505920.0000\n",
      "Epoch 142/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 162117844992.0000\n",
      "Epoch 143/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 159924060160.0000\n",
      "Epoch 144/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 157741088768.0000\n",
      "Epoch 145/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 155573616640.0000\n",
      "Epoch 146/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 153422069760.0000\n",
      "Epoch 147/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 151294853120.0000\n",
      "Epoch 148/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 149183741952.0000\n",
      "Epoch 149/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 147083935744.0000\n",
      "Epoch 150/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 145004789760.0000\n",
      "Epoch 151/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 142945058816.0000\n",
      "Epoch 152/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 140894388224.0000\n",
      "Epoch 153/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 138872733696.0000\n",
      "Epoch 154/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 136863981568.0000\n",
      "Epoch 155/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 134880174080.0000\n",
      "Epoch 156/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 132911964160.0000\n",
      "Epoch 157/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 130969919488.0000\n",
      "Epoch 158/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 129042006016.0000\n",
      "Epoch 159/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 127140265984.0000\n",
      "Epoch 160/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 125259767808.0000\n",
      "Epoch 161/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 123401453568.0000\n",
      "Epoch 162/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 121562603520.0000\n",
      "Epoch 163/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 119750172672.0000\n",
      "Epoch 164/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 117963276288.0000\n",
      "Epoch 165/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 116187447296.0000\n",
      "Epoch 166/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 114439806976.0000\n",
      "Epoch 167/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 112714907648.0000\n",
      "Epoch 168/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 111012315136.0000\n",
      "Epoch 169/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 109331636224.0000\n",
      "Epoch 170/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 107677761536.0000\n",
      "Epoch 171/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 106049462272.0000\n",
      "Epoch 172/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 104436449280.0000\n",
      "Epoch 173/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 102853255168.0000\n",
      "Epoch 174/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 101294915584.0000\n",
      "Epoch 175/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 99758473216.0000\n",
      "Epoch 176/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 98244567040.0000\n",
      "Epoch 177/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 96757514240.0000\n",
      "Epoch 178/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 95296954368.0000\n",
      "Epoch 179/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 93855834112.0000\n",
      "Epoch 180/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 92444401664.0000\n",
      "Epoch 181/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 91051433984.0000\n",
      "Epoch 182/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 89690300416.0000\n",
      "Epoch 183/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 88354856960.0000\n",
      "Epoch 184/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 87040794624.0000\n",
      "Epoch 185/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 85756379136.0000\n",
      "Epoch 186/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 84489175040.0000\n",
      "Epoch 187/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 83249438720.0000\n",
      "Epoch 188/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 82042814464.0000\n",
      "Epoch 189/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 80856768512.0000\n",
      "Epoch 190/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 79696224256.0000\n",
      "Epoch 191/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 78561517568.0000\n",
      "Epoch 192/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 77452722176.0000\n",
      "Epoch 193/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 76363186176.0000\n",
      "Epoch 194/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 75299430400.0000\n",
      "Epoch 195/200\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 74268418048.0000\n",
      "Epoch 196/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 73262112768.0000\n",
      "Epoch 197/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 72274599936.0000\n",
      "Epoch 198/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 71316873216.0000\n",
      "Epoch 199/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 70382616576.0000\n",
      "Epoch 200/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 69469749248.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f312c022940>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit(\n",
    "    normalized_x_train,y_train,\n",
    "    epochs=100,\n",
    "    batch_size=10,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step\n",
      "Mean Squared Error: 168728682060.96075\n"
     ]
    }
   ],
   "source": [
    "x_test, y_test = generate_housing_data(num_points=50)  # Generate 50 test data points\n",
    "\n",
    "# Use the model to predict prices for the test data\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# Compute the mean squared error between the predicted prices and the actual prices\n",
    "mse = np.mean((y_pred - y_test) ** 2)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.06512558 2.5       ]\n",
      "[3258    3]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Predicted Price: 503979.97\n",
      "Actual Price:    761972.7101015416\n",
      "Difference:      257992.7413515416\n",
      "[0.04334297 5.        ]\n",
      "[539   5]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "Predicted Price: 451233.47\n",
      "Actual Price:    213368.7981574547\n",
      "Difference:      237864.6705925453\n",
      "[0.33451878 5.        ]\n",
      "[801   5]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "Predicted Price: 474132.38\n",
      "Actual Price:    199397.8414378474\n",
      "Difference:      274734.5335621526\n",
      "[2.27606135 5.        ]\n",
      "[2548    5]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Predicted Price: 626820.9\n",
      "Actual Price:    622930.9822561208\n",
      "Difference:      3889.8927438792307\n",
      "[4.25650144 0.        ]\n",
      "[4330    1]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted Price: 412778.06\n",
      "Actual Price:    893632.757431946\n",
      "Difference:      480854.694931946\n"
     ]
    }
   ],
   "source": [
    "ind = 1\n",
    "# for loop 5\n",
    "for ind in range(5):\n",
    "    \n",
    "\n",
    "    print(normalized_x_train[ind])\n",
    "    print(x_train[ind])\n",
    "    # model.predict(x_train[ind])\n",
    "\n",
    "    x = np.reshape(normalized_x_train[ind], (1, 2))\n",
    "    y_pred = model.predict(x)\n",
    "\n",
    "    print(\"Predicted Price:\", y_pred[0][0])\n",
    "    print(\"Actual Price:   \", y_train[ind])\n",
    "    print(\"Difference:     \", abs(y_pred[0][0] - y_train[ind]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
